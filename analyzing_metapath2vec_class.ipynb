{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing Pytorch Geometric Metapath2vec class\n",
    "- [source code](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/models/metapath2vec.html)  \n",
    "- [docs](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.models.MetaPath2Vec)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "import torch\n",
    "from torch_geometric.typing import NodeType, EdgeType, OptTensor\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "EPS = 1e-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('paper', 'written_by', 'author') [2, 9323605]\n",
      "('author', 'writes', 'paper') [2, 9323605]\n",
      "('paper', 'published_in', 'venue') [2, 3194405]\n",
      "('venue', 'publishes', 'paper') [2, 3194405]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch_geometric.datasets import AMiner\n",
    "\n",
    "path = os.path.join(os.getcwd(), 'data/AMiner')\n",
    "dataset = AMiner(path)\n",
    "data = dataset[0]\n",
    "\n",
    "# keys = metapath\n",
    "for k, v in data.edge_index_dict.items():\n",
    "    print(k, list(v.shape))\n",
    "\n",
    "metapath = [\n",
    "    ('author', 'writes', 'paper'),\n",
    "    ('paper', 'published_in', 'venue'),\n",
    "    ('venue', 'publishes', 'paper'),\n",
    "    ('paper', 'written_by', 'author'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    edge_index_dict (Dict[Tuple[str, str, str], Tensor]): Dictionary\n",
    "        holding edge indices for each\n",
    "        :obj:`(src_node_type, rel_type, dst_node_type)` present in the heterogeneous graph.\n",
    "    embedding_dim (int): The size of each embedding vector.\n",
    "\n",
    "    metapath (List[Tuple[str, str, str]]): The metapath described as a list\n",
    "        of :obj:`(src_node_type, rel_type, dst_node_type)` tuples.\n",
    "    \n",
    "    walk_length (int):\n",
    "        The random walk length. Ex) 100\n",
    "\n",
    "    context_size (int):\n",
    "        = window size\n",
    "        The actual context size which is considered for\n",
    "        positive samples. This parameter increases the effective sampling\n",
    "        rate by reusing samples across different source nodes.\n",
    "\n",
    "    walks_per_node (int, optional): The number of walks to sample for each\n",
    "        node. (default: :obj:`1`)\n",
    "\n",
    "    num_negative_samples (int, optional): The number of negative samples to\n",
    "        use for each positive sample. (default: :obj:`1`)\n",
    "\n",
    "    num_nodes_dict (Dict[str, int], optional):\n",
    "        Dictionary holding the number of nodes for each node type.\n",
    "        (default: :obj:`None`)\n",
    "        Ex) {'paper': 3194405, 'author': 1693531, 'venue': 3883}\n",
    "\n",
    "    sparse (bool, optional): If set to :obj:`True`, gradients w.r.t. to the\n",
    "        weight matrix will be sparse. (default: :obj:`False`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set arguments for example\n",
    "edge_index_dict = data.edge_index_dict\n",
    "\n",
    "embedding_dim = 128\n",
    "walk_length = 50\n",
    "context_size = 7\n",
    "walks_per_node = 5\n",
    "num_negative_samples = 5\n",
    "sparse = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create num_nodes_dict if None\n",
    "num_nodes_dict = {}\n",
    "for keys, edge_index in edge_index_dict.items():\n",
    "    key = keys[0]\n",
    "    N = int(edge_index[0].max() + 1)\n",
    "    num_nodes_dict[key] = max(N, num_nodes_dict.get(key, N))\n",
    "\n",
    "    key = keys[-1]\n",
    "    N = int(edge_index[1].max() + 1)\n",
    "    num_nodes_dict[key] = max(N, num_nodes_dict.get(key, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paper': 3194405, 'author': 1693531, 'venue': 3883}\n"
     ]
    }
   ],
   "source": [
    "print(num_nodes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create adj_dict based on each metapath\n",
    "adj_dict = {}\n",
    "for keys, edge_index in edge_index_dict.items():\n",
    "    sizes = (num_nodes_dict[keys[0]], num_nodes_dict[keys[-1]])\n",
    "    row, col = edge_index\n",
    "    adj = SparseTensor(row=row, col=col, sparse_sizes=sizes)\n",
    "    adj = adj.to('cpu')\n",
    "    adj_dict[keys] = adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('paper',\n",
       "  'written_by',\n",
       "  'author'): SparseTensor(row=tensor([      0,       1,       2,  ..., 3194404, 3194404, 3194404]),\n",
       "              col=tensor([     0,      1,      2,  ...,   4393,  21681, 317436]),\n",
       "              size=(3194405, 1693531), nnz=9323605, density=0.00%),\n",
       " ('author',\n",
       "  'writes',\n",
       "  'paper'): SparseTensor(row=tensor([      0,       0,       0,  ..., 1693528, 1693529, 1693530]),\n",
       "              col=tensor([      0,   45988,  124807,  ..., 3194371, 3194387, 3194389]),\n",
       "              size=(1693531, 3194405), nnz=9323605, density=0.00%),\n",
       " ('paper',\n",
       "  'published_in',\n",
       "  'venue'): SparseTensor(row=tensor([      0,       1,       2,  ..., 3194402, 3194403, 3194404]),\n",
       "              col=tensor([2190, 2190, 2190,  ..., 3148, 3148, 3148]),\n",
       "              size=(3194405, 3883), nnz=3194405, density=0.03%),\n",
       " ('venue',\n",
       "  'publishes',\n",
       "  'paper'): SparseTensor(row=tensor([   0,    0,    0,  ..., 3882, 3882, 3882]),\n",
       "              col=tensor([2203069, 2203070, 2203071,  ...,  952391,  952392,  952393]),\n",
       "              size=(3883, 3194405), nnz=3194405, density=0.03%)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(adj_dict)\n",
    "# P A -> (num_papers, num_authors) = (3194405, 1693531)\n",
    "# P V -> (num_papers, num_venues) = (3194405, 3883)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['author', 'paper', 'venue']\n"
     ]
    }
   ],
   "source": [
    "types = set([x[0] for x in metapath]) | set([x[-1] for x in metapath])\n",
    "types = sorted(list(types))\n",
    "print(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporate different types into one long line\n",
    "# author: 0 ~ 1693531\n",
    "# paper : 1693531 ~ 4887936\n",
    "# venue: 4887936 ~ 4891819\n",
    "count = 0\n",
    "start, end = {}, {}\n",
    "for key in types:\n",
    "    start[key] = count\n",
    "    count += num_nodes_dict[key]\n",
    "    end[key] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('author', 'writes', 'paper'), ('paper', 'published_in', 'venue'), ('venue', 'publishes', 'paper'), ('paper', 'written_by', 'author')]\n"
     ]
    }
   ],
   "source": [
    "print(metapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set offset\n",
    "# start point of author\n",
    "offset = [start[metapath[0][0]]]    # [0]\n",
    "\n",
    "# add start points of paper, venue, paper, author\n",
    "# [0, 1693531, 4887936, 1693531, 0] = offset\n",
    "offset += [start[keys[-1]] for keys in metapath]\n",
    "\n",
    "# repeat offset 15times -> length: 65\n",
    "offset = offset * int((walk_length / len(metapath)) + 1)\n",
    "\n",
    "offset = offset[:walk_length + 1]    # only use up to walk_length+1\n",
    "assert len(offset) == walk_length + 1    # length: 51\n",
    "offset = torch.tensor(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store every embedding of nodes\n",
    "# + 1 denotes a dummy node used to link to for isolated nodes.\n",
    "embedding = Embedding(count + 1, embedding_dim, sparse=sparse)\n",
    "dummy_idx = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 128]\n"
     ]
    }
   ],
   "source": [
    "# forward method: return batch embeddings\n",
    "# Returns the embeddings for the nodes in batch of type node_type\n",
    "# Ex) start['paper'], end['paper'] = (1693531, 4887936)\n",
    "\n",
    "# node_type: str, batch: OptTensor\n",
    "# index_select method ref: https://pytorch.org/docs/stable/generated/torch.index_select.html\n",
    "\n",
    "emb = embedding.weight[start[node_type]:end[node_type]]\n",
    "batch = torch.LongTensor([0,1,2,3,4,5,6,7])\n",
    "output = emb if batch is None else emb.index_select(dim=0, index=batch)\n",
    "print(list(output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Order: sample function -> _pos_sample, _neg_sample -> _sample -> loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(src: SparseTensor, subset: Tensor, num_neighbors: int, dummy_idx: int) -> Tensor:\n",
    "    # sparse tensor는 row <-> col의 상호관계를 명시한다.\n",
    "    # src에서 batch에 해당하는 row가 상호작용한 col 중에서 num_neighbors 만큼 각각 샘플링한다.\n",
    "    # 만약 상호작용한 결과가 존재하지 않는다면 dummy index를 추출한다.\n",
    "    # subset: an index that determines which part of src to take as a sample\n",
    "    # col: result of extracting the interacting elements for each batch element\n",
    "\n",
    "    mask = subset < dummy_idx\n",
    "    rowcount = torch.zeros_like(subset)\n",
    "\n",
    "    # if size of SparseTensor: (r, c)\n",
    "    # then SparseTensor.storage.rowcount() shape: (r, )\n",
    "    # meaning rowcount represents the number of edges b/w each row and every column\n",
    "    # sample of rowcount\n",
    "    rowcount[mask] = src.storage.rowcount()[subset[mask]]\n",
    "    mask = mask & (rowcount > 0)\n",
    "\n",
    "    # sample of rowptr\n",
    "    offset = torch.zeros_like(subset)\n",
    "    offset[mask] = src.storage.rowptr()[subset[mask]]\n",
    "\n",
    "    # Ex)\n",
    "    # rowcount: ([ 32, 101,  64, 120,  25,  23,  90,  63])\n",
    "    # -> 0 interacts with 32 neighbors\n",
    "    # offset: ([  0,  32, 133, 197, 317, 342, 365, 455])\n",
    "    # -> 0: 0~31, 1: 32~132, ... in src.storage.row() = rowptr\n",
    "    rand = torch.rand((rowcount.size(0), num_neighbors), device=subset.device)    # Ex: (8, 1)\n",
    "    rand.mul_(rowcount.to(rand.dtype).view(-1, 1))\n",
    "    rand = rand.to(torch.long)    # Ex: [18, 92, 16, 102, 14, 12, 28, 15]\n",
    "    rand.add_(offset.view(-1, 1))\n",
    "\n",
    "    col = src.storage.col()[rand]\n",
    "    col[~mask] = dummy_idx    # Ex: [739183, 2803719, 1982397, 633236, 1222151, 1562057, 323228, 2651545]\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample example\n",
    "adj = adj_dict[('author', 'writes', 'paper')]\n",
    "b = sample(src=adj, subset=batch, num_neighbors=1, dummy_idx=dummy_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pos_sample method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.LongTensor([0,1,2,3,4,5,6,7])\n",
    "batch = batch.repeat(walks_per_node)    # batch X walks_per_node\n",
    "\n",
    "# random walks\n",
    "rws = [batch]\n",
    "for i in range(walk_length):\n",
    "    keys = metapath[i % len(metapath)]\n",
    "    adj = adj_dict[keys]\n",
    "    # prev batch becomes input of sample function inside the loop -> random walk\n",
    "    batch = sample(adj, batch, num_neighbors=1, dummy_idx=dummy_idx).view(-1)\n",
    "    rws.append(batch)\n",
    "\n",
    "# stack\n",
    "rw = torch.stack(rws, dim=-1)    # (batch_size*walks_per_node, walk_length)\n",
    "\n",
    "# follow the pre-defined metapath by adding offset: ([0, 1693531, 4887936, 1693531, 0])\n",
    "rw.add_(offset.view(1, -1))\n",
    "rw[rw > dummy_idx] = dummy_idx    # if index is greater than \"count\", change it to \"count\"\n",
    "\n",
    "# chunk by context_size\n",
    "walks = []\n",
    "num_walks_per_rw = 1 + walk_length + 1 - context_size\n",
    "\n",
    "for j in range(num_walks_per_rw):\n",
    "    walks.append(rw[:, j:j+context_size])\n",
    "output = torch.cat(walks, dim=0)    # (batch_size * walks_per_node * num_walks_per_rw, context_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- neg_sample method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same way as pos_sample method\n",
    "batch = torch.LongTensor([0,1,2,3,4,5,6,7])\n",
    "batch = batch.repeat(walks_per_node * num_negative_samples)\n",
    "\n",
    "rws = [batch]\n",
    "for i in range(walk_length):\n",
    "    keys = metapath[i % len(metapath)]\n",
    "    batch = torch.randint(low=0, high=num_nodes_dict[keys[-1]], size=(batch.size(0), ), dtype=torch.long)\n",
    "    rws.append(batch)\n",
    "\n",
    "rw = torch.stack(rws, dim=-1)\n",
    "rw.add_(offset.view(1, -1))\n",
    "\n",
    "walks = []\n",
    "num_walks_per_rw = 1 + walk_length + 1 - context_size\n",
    "for j in range(num_walks_per_rw):\n",
    "    walks.append(rw[:, j:j + context_size])\n",
    "output = torch.cat(walks, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _sample method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample(self, batch: List[int]) -> Tuple[Tensor, Tensor]:\n",
    "    if not isinstance(batch, Tensor):\n",
    "        batch = torch.tensor(batch, dtype=torch.long)\n",
    "    return _pos_sample(batch), _neg_sample(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loader method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn\n",
    "# merges a list of samples to form a mini-batch of Tensor(s). \n",
    "# Used when using batched loading from a map-style dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(self, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns the data loader\n",
    "    that creates both positive and negative random walks on the heterogeneous graph.\n",
    "\n",
    "    **kwargs (optional):\n",
    "        Arguments of torch.utils.data.DataLoader\n",
    "        such as batch_size, shuffle, drop_last or num_workers\n",
    "    \"\"\"\n",
    "    # starts with the beginning of metapath\n",
    "    return DataLoader(range(self.num_nodes_dict[self.metapath[0][0]]), collate_fn=_sample, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loss, test method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function is as follows:\n",
    "\n",
    "$$ log \\sigma (X_{c_t} \\cdot X_v) + \\Sigma_{m=1}^M [1 - log \\sigma(X_{u^m} \\cdot X_v)] $$  \n",
    "\n",
    "$v$ is start node, $M$ means the number of negative nodes drawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(self, pos_rw: Tensor, neg_rw: Tensor) -> Tensor:\n",
    "    # Computes the loss given positive and negative random walks.\n",
    "\n",
    "    # Positive loss.\n",
    "    start, rest = pos_rw[:, 0], pos_rw[:, 1:].contiguous()\n",
    "\n",
    "    h_start = self.embedding(start).view(\n",
    "        pos_rw.size(0), 1, self.embedding_dim)\n",
    "    h_rest = self.embedding(rest.view(-1)).view(\n",
    "        pos_rw.size(0), -1, self.embedding_dim)\n",
    "\n",
    "    out = (h_start * h_rest).sum(dim=-1).view(-1)\n",
    "    pos_loss = -torch.log(torch.sigmoid(out) + EPS).mean()\n",
    "\n",
    "    # Negative loss.\n",
    "    start, rest = neg_rw[:, 0], neg_rw[:, 1:].contiguous()\n",
    "\n",
    "    h_start = self.embedding(start).view(\n",
    "        neg_rw.size(0), 1, self.embedding_dim)\n",
    "    h_rest = self.embedding(rest.view(-1)).view(\n",
    "        neg_rw.size(0), -1, self.embedding_dim)\n",
    "\n",
    "    out = (h_start * h_rest).sum(dim=-1).view(-1)\n",
    "    neg_loss = -torch.log(1 - torch.sigmoid(out) + EPS).mean()\n",
    "\n",
    "    return pos_loss + neg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(self, train_z: Tensor, train_y: Tensor, test_z: Tensor,\n",
    "            test_y: Tensor, solver: str = \"lbfgs\", multi_class: str = \"auto\",\n",
    "            *args, **kwargs) -> float:\n",
    "    # Evaluates latent space quality via a logistic regression downstream task.\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    clf = LogisticRegression(solver=solver, multi_class=multi_class, *args,\n",
    "                                **kwargs).fit(train_z.detach().cpu().numpy(),\n",
    "                                            train_y.detach().cpu().numpy())\n",
    "    return clf.score(test_z.detach().cpu().numpy(),\n",
    "                        test_y.detach().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a41dfc5d5a1fc57aabcbc0d089c03a7c30314879472359c6451c143ff21a585"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
